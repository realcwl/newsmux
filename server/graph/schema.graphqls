# GraphQL schema

enum FeedRefreshDirection {
  NEW
  OLD
}

enum Visibility {
  GLOBAL
  PRIVATE
}

input SourcesInput {
  subSourceFromSharedPost: Boolean!
}

input SubsourcesInput {
  isFromSharedPost: Boolean!
}

input PostInput {
  id: String!
}

input NewUserInput {
  id: String!
  name: String!
}

input UpsertFeedInput {
  userId: String!
  feedId: String
  name: String!
  filterDataExpression: String!
  subSourceIds: [String!]!
  visibility: Visibility!
}

# TODO: for testing purpose, real post is created by crawler and publisher
input NewPostInput {
  title: String!
  content: String!
  subSourceId: String!
  feedsIdPublishTo: [String!]
  sharedFromPostId: String
}

input SubscribeInput {
  userId: String!
  feedId: String!
}

input NewSourceInput {
  userId: String!
  name: String!
  domain: String!
  customizedCrawlerPanopticConfigForm: CustomizedCrawlerPanopticConfigForm
}

# This schema has all information needed to construct a PanopticConfig in panoptic_config.proto
# data_collector_id is predefined to be COLLECTOR_USER_CUSTOMIZED_SOURCE
input CustomizedCrawlerPanopticConfigForm {
  name: String # name of the config if not specified, use source name

  # for TaskSchedule:
  startImmediately: Boolean # default to true
  scheduleEveryMilliseconds: Int # default to 5 minutes

  # for CustomizedCrawlerParams
  customizedCrawlerParams: CustomizedCrawlerParams!
}

# This schema has all information needed to construct a CustomizedCrawlerParams in panoptic.proto
input CustomizedCrawlerParams {
  crawlUrl: String! # url to crawl, e.g. https://www.cls.cn/telegraph
  baseSelector: String! # base selector should return a list of DOM elements where each one corresponds to a single post
  titleRelativeSelector: String # relative selector to the base selector
  contentRelativeSelector: String
  externalIdRelativeSelector: String
  timeRelativeSelector: String #if not specified, use the cralwed time as content generated time
  imageRelativeSelector: String
  subsourceRelativeSelector: String #how to deal with subsource spec
  originUrlRelativeSelector: String #by default is the url
}

# isFromSharedPost = true means the subsource is not for cralwing
# it is from a shared post
# example: when subsource is an owner of a post in a retweet post
input UpsertSubSourceInput {
  # disable subsource id, use name + source id to identify
  # subSourceId: String
  name: String!
  externalIdentifier: String!
  sourceId: String!
  avatarUrl: String!
  originUrl: String!
  isFromSharedPost: Boolean!

  # use this to customize crawler behavior, the source should have 
  # collector_id = COLLECTOR_USER_CUSTOMIZED_SUBSOURCE in config
  customizedCrawlerParams: CustomizedCrawlerParams
}

# Add weibo user to the database for panoptic to crawl
input AddWeiboSubSourceInput {
  name: String!
}

input AddSubSourceInput {
  # SourceId to which this subSource belongs to.
  sourceId: String!
  # subSourceUserName is the actual username for a given Source.
  subSourceUserName: String!
}

input FeedRefreshInput {
  feedId: String!
  limit: Int!
  cursor: Int!
  direction: FeedRefreshDirection!
  feedUpdatedTime: Time
}

input FeedsGetPostsInput {
  userId: String!
  feedRefreshInputs: [FeedRefreshInput!]!
}

input DeleteFeedInput {
  userId: String!
  feedId: String!
}

type Query {
  allVisibleFeeds: [Feed!]
  post(input: PostInput): Post!
  posts: [Post!]
  users: [User!]

  # State is the main API to "bootstrap" application, where it fetches required
  # states for the given input. After receiving the StateOutput, client will
  # then make subsequent calls to request all data.
  userState(input: UserStateInput!): UserState!

  # Feeds is the main API for newsfeed
  # WARNING: if you do not pass feedUpdatedTime, your curosr/direction will be ignored
  # WARNING: please pass the cursor based on returned posts cursor, otherwise the republish will skip some posts

  # It is used to return posts for a feed
  # It can be called with a list of following queries, each query represent a feed
  # Caller can specify only 1 or more feeds

  # FeedID          string					Feed id to fetch posts
  # Limit           int						Max amount of posts shall the API return, at most 30
  # Cursor          int						The cursor of the pivot post
  # Direction       FeedRefreshDirection	NEW or OLD description below
  # FeedUpdatedTime *time.Time				Time stamp used to represent feed version description below

  # Returns: Feeds
  # 	caller can get each feed's FeedUpdatedTime and its posts, and each post has a cursor

  # How to use cursor and direction?
  # 	Direction = NEW:    load feed new posts with cursor larger than cursor A (default -1), from newest one, no more than Limit
  # 	Direction = OLD: load feed old posts with cursor smaller than cursor B (default -1), from newest one, no more than Limit

  # 	If not specified, use NEW as direction, -1 as cursor to give newest Posts

  # 	How is cursor defined:
  # 		it is an auto-increament index Posts

  # What if feed is changed, and front end doesn't know?
  # Feed updates are not pushed to frontend, backend pass a turn-around field FeedUpdatedTime
  # 	to frontend, and API call will carry this timestamp. Once feed is updated, API will know the
  # 	input timestamp is not same as the updated "FeedUpdatedTime", thus, will not respect the cursor

  # If frontend disconnected for a while, how do front end know it gets all posts up until latest?
  # In this case, front end can still query {Feeds} with its stored NEWest cursor
  # 	{Feeds} will return the most recent N post up to N=Limit
  # 	Front end can check if the N == Limit, if so, it indicate there is very likely to be more posts
  # 	need to be fetched. And frontend can send another {Feeds} request. It can also choose not
  # 	to fetch so many posts.

  # What if the data expression filter or subsource are changed
  # Chaging feed is on {upsertFeed}
  # For {feeds} we will handle on-the-fly posts re-publish, in these conditions:
  # 1. query OLD but can't satisfy the limit
  feeds(input: FeedsGetPostsInput): [Feed!]!
  subSources(input: SubsourcesInput): [SubSource!]!
  sources(input: SourcesInput): [Source!]

  tryCustomizedCrawler(input: CustomizedCrawlerParams): [CustomizedCrawlerTestResponse!]
}

type Mutation {
  createUser(input: NewUserInput!): User!
  upsertFeed(input: UpsertFeedInput!): Feed!
  deleteFeed(input: DeleteFeedInput!): Feed!
  # TODO: for testing purpose, real post is created by crawler and publisher
  createPost(input: NewPostInput!): Post!
  # TODO: what should be a better output
  subscribe(input: SubscribeInput!): User!

  createSource(input: NewSourceInput!): Source!
  upsertSubSource(input: UpsertSubSourceInput!): SubSource!

  # Deprecated!
  # TODO(chenweilunster): Remove this function and all its existence.
  addWeiboSubSource(input: AddWeiboSubSourceInput!): SubSource!

  # For now, addSubSource is used when frontend wants to add some more
  # subSources for a given source (e.g. Weibo, Twitter). It should validate the
  # input, store the normalized version, and return the error code if any.
  # This mutation isn't intended to be used as a generic AddSubSource method for
  # now, but it can be extended to be a generic one.
  addSubSource(input: AddSubSourceInput!): SubSource!

  syncUp(input: SeedStateInput): SeedState
}

type Subscription {
  # Subscribe to signals sending from server side. Client side should handle
  # signals properly. The first time this is called will always return
  # SEEDSTATE signal.
  signal(userId: String!): Signal!
}

scalar Time
